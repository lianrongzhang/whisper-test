#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å½±ç‰‡çš„é€å­—ç¨¿ï¼ˆ2024å¹´1æœˆè‡³2025å¹´10æœˆï¼‰
ä» data ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶å¤¹ä¸­çš„ video_details æ–‡ä»¶è¯»å–å½±ç‰‡ä¿¡æ¯
"""

import csv
import json
import re
import subprocess
from collections import defaultdict
from datetime import datetime
from pathlib import Path

from tqdm import tqdm
from youtube_transcribe import YouTubeTranscriber


# ============================================================
# è¾…åŠ©å‡½æ•°ï¼šå­—å¹•å¤„ç†
# ============================================================

def clean_repeated_lines(lines):
    """
    å»é™¤è¿ç»­é‡å¤çš„é€å­—ç¨¿è¡Œ
    
    Args:
        lines: åŸå§‹é€å­—ç¨¿è¡Œåˆ—è¡¨
        
    Returns:
        å»é‡åçš„è¡Œåˆ—è¡¨
    """
    cleaned = []
    last_text = None

    for line in lines:
        m = re.match(r"\[\d+:\d+\.\d+ --> .*?\]\s*(.*)", line)
        text = m.group(1).strip() if m else line.strip()
        if text == last_text:
            continue  # è·³è¿‡è¿ç»­é‡å¤
        cleaned.append(line)
        last_text = text

    return cleaned


def clean_transcript_file(path: Path):
    """
    è¯»å–é€å­—ç¨¿æ–‡ä»¶ï¼Œå»é™¤è¿ç»­é‡å¤è¡Œï¼Œå¹¶è¦†å†™åŸæ–‡ä»¶
    
    Args:
        path: é€å­—ç¨¿æ–‡ä»¶è·¯å¾„
    """
    with open(path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    cleaned_lines = clean_repeated_lines(lines)

    with open(path, "w", encoding="utf-8") as f:
        f.writelines(cleaned_lines)


def get_manual_subtitles(video_url):
    cmd = ["yt-dlp", "-J", video_url]
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        return {}

    info = json.loads(result.stdout)
    subs = info.get("subtitles", {}) or {}

    # åªä¿ç•™ä¸­æ–‡å­—å¹•
    zh_keys = [k for k in subs.keys() if k.startswith("zh")]
    return {k: subs[k] for k in zh_keys}


def download_manual_subtitle(video_url, output_stem: Path):
    """
    ä¸‹è½½çœŸäººå­—å¹•ï¼ˆvttï¼‰ï¼Œä¸ä¸‹è½½è‡ªåŠ¨å­—å¹•
    
    Args:
        video_url: YouTube è§†é¢‘ URL
        output_stem: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    """
    cmd = [
        "yt-dlp",
        "--skip-download",
        "--write-subs",
        "--no-write-auto-subs",
        "--sub-lang", "zh,zh-Hant,zh-TW",
        "--sub-format", "vtt",
        "-o", str(output_stem),
        video_url
    ]
    subprocess.run(cmd, check=True)


def vtt_to_txt(vtt_path: Path, txt_path: Path):
    """
    å°† VTT å­—å¹•è½¬ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼ˆä¿ç•™æ—¶é—´æˆ³ï¼‰
    
    Args:
        vtt_path: VTT å­—å¹•æ–‡ä»¶è·¯å¾„
        txt_path: è¾“å‡ºçš„ TXT æ–‡ä»¶è·¯å¾„
    """
    with open(vtt_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    output = []
    for line in lines:
        line = line.strip()
        if not line or line.startswith("WEBVTT"):
            continue
        output.append(line + "\n")

    with open(txt_path, "w", encoding="utf-8") as f:
        f.writelines(output)


# ============================================================
# ä¸»ç±»ï¼šæ‰¹é‡è½¬å½•å™¨
# ============================================================

class AllVideoTranscriber:
    def __init__(self, data_dir, transcripts_dir="transcripts_all", 
                 start_date="2024-01-01", end_date="2025-10-31"):
        """
        åˆå§‹åŒ–æ‰¹é‡é€å­—ç¨¿ç”Ÿæˆå™¨
        
        Args:
            data_dir: dataç›®å½•ï¼ˆåŒ…å«æ‰€æœ‰é¢‘é“æ–‡ä»¶å¤¹ï¼‰
            transcripts_dir: é€å­—ç¨¿è¾“å‡ºç›®å½•
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        self.data_dir = Path(data_dir)
        self.transcripts_dir = Path(transcripts_dir)
        self.transcripts_dir.mkdir(exist_ok=True)
        
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d")

        
        # è§†é¢‘ä¿¡æ¯
        self.video_info = defaultdict(lambda: {
            'video_id': '',
            'video_title': '',
            'channel': '',
            'channel_title': '',
            'publish_date': '',
            'view_count': 0,
            'like_count': 0,
            'comment_count': 0,
            'duration': '',
            'description': '',
            'transcript_source': ''
        })
        
        # ç»Ÿè®¡
        self.stats = {
            'total_video_details_files': 0,
            'total_videos_found': 0,
            'videos_in_date_range': 0,
            'videos_with_transcript': 0,
            'videos_need_transcript': 0
        }

    def load_videos_from_sample_json(self, sample_json_path):
        """
        ä»ç­›é€‰åçš„ JSON æ ·æœ¬æ–‡ä»¶è¯»å–å½±ç‰‡ä¿¡æ¯
        JSON ç»“æ„éœ€åŒ…å« videos[].url æˆ– videos[].video_id
        """
        print("=" * 80)
        print("æ­¥éª¤ 1: ä»ç­›é€‰æ ·æœ¬ JSON åŠ è½½å½±ç‰‡")
        print("=" * 80)

        with open(sample_json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        videos = data.get("videos", [])
        print(f"\nè¯»å–åˆ° {len(videos):,} ä¸ªç­›é€‰åå½±ç‰‡")

        for video in videos:
            video_id = video.get("video_id")
            url = video.get("url")

            if not video_id and url:
                # fallbackï¼šä» url è§£æ video_id
                if "v=" in url:
                    video_id = url.split("v=")[-1].split("&")[0]

            if not video_id:
                continue

            # è§£æå‘å¸ƒæ—¥æœŸï¼ˆå¯é€‰ï¼‰
            publish_date = video.get("published_at", "")
            if publish_date:
                try:
                    video_date = datetime.fromisoformat(
                        publish_date.replace("Z", "+00:00")
                    ).replace(tzinfo=None)
                    if not (self.start_date <= video_date <= self.end_date):
                        continue
                except Exception:
                    pass

            info = self.video_info[video_id]
            info["video_id"] = video_id
            info["video_title"] = video.get("title", "")
            info["channel"] = video.get("channel", "")
            info["channel_title"] = video.get("channel", "")
            info["publish_date"] = publish_date
            info["view_count"] = int(video.get("view_count", 0))
            info["like_count"] = int(video.get("like_count", 0))
            info["comment_count"] = int(video.get("comment_count", 0))
            info["description"] = video.get("description", "")
            info["url"] = video.get("url")

        print(f"\nâœ… æˆåŠŸåŠ è½½ {len(self.video_info):,} ä¸ªå½±ç‰‡ï¼ˆæ¥è‡ªæ ·æœ¬ JSONï¼‰")
        return len(self.video_info)
    
    def load_all_videos(self):
        """ä» data ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶å¤¹ä¸­çš„ video_details æ–‡ä»¶åŠ è½½å½±ç‰‡ä¿¡æ¯"""
        print("="*80)
        print("æ­¥éª¤ 1: ä» data ç›®å½•åŠ è½½æ‰€æœ‰å½±ç‰‡ä¿¡æ¯")
        print("="*80)
        
        print(f"\næ‰«æç›®å½•: {self.data_dir}")
        print(f"æ—¥æœŸèŒƒå›´: {self.start_date.strftime('%Y-%m-%d')} è‡³ {self.end_date.strftime('%Y-%m-%d')}")
        
        # æŸ¥æ‰¾æ‰€æœ‰ video_details æ–‡ä»¶
        detail_files = list(self.data_dir.glob("**/video_details_*.json"))
        self.stats['total_video_details_files'] = len(detail_files)
        
        print(f"\næ‰¾åˆ° {len(detail_files)} ä¸ª video_details æ–‡ä»¶")
        
        if len(detail_files) == 0:
            print("\nâŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½• video_details æ–‡ä»¶")
            print("   è¯·ç¡®è®¤ data ç›®å½•ç»“æ„æ­£ç¡®")
            return 0
        
        # åŠ è½½æ‰€æœ‰å½±ç‰‡ä¿¡æ¯
        print("\næ­£åœ¨åŠ è½½å½±ç‰‡ä¿¡æ¯...")
        
        for detail_file in tqdm(detail_files, desc="åŠ è½½ video_details"):
            try:
                with open(detail_file, 'r', encoding='utf-8') as f:
                    videos = json.load(f)
                    
                    for video in videos:
                        video_id = video.get('id', '')
                        if not video_id:
                            continue
                        
                        self.stats['total_videos_found'] += 1
                        
                        # è§£æå‘å¸ƒæ—¥æœŸ
                        snippet = video.get("snippet", {})
                        publish_date = snippet.get('publishedAt', '')
                        if not publish_date:
                            continue

                        try:
                            video_date = datetime.fromisoformat(
                                publish_date.replace("Z", "+00:00")
                            ).replace(tzinfo=None)
                        except ValueError:
                            continue
                        
                        if not (self.start_date <= video_date <= self.end_date):
                            continue
                        
                        self.stats['videos_in_date_range'] += 1
                        
                        # æå–é¢‘é“ä¿¡æ¯ï¼ˆä»æ–‡ä»¶è·¯å¾„ï¼‰
                        channel_id = detail_file.parent.name
                        
                        # ä¿å­˜å½±ç‰‡ä¿¡æ¯
                        info = self.video_info[video_id]
                        info['video_id'] = video_id
                        info['video_title'] = snippet.get('title', '')
                        info['channel'] = channel_id
                        info['channel_title'] = snippet.get('channelTitle', '')
                        info['publish_date'] = publish_date
                        info['view_count'] = int(video.get('viewCount', 0))
                        info['like_count'] = int(video.get('likeCount', 0))
                        info['comment_count'] = int(video.get('commentCount', 0))
                        info['duration'] = video.get('duration', '')
                        info['description'] = video.get('description', '')
                        
            except Exception as e:
                print(f"\nâš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥: {detail_file.name} - {e}")
                continue
        
        print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  æ‰«æçš„ video_details æ–‡ä»¶: {self.stats['total_video_details_files']:,}")
        print(f"  æ‰¾åˆ°çš„æ€»å½±ç‰‡æ•°: {self.stats['total_videos_found']:,}")
        print(f"  æ—¥æœŸèŒƒå›´å†…çš„å½±ç‰‡: {self.stats['videos_in_date_range']:,}")
        print(f"  å”¯ä¸€å½±ç‰‡ID: {len(self.video_info):,}")
        
        return len(self.video_info)
    
    def check_existing_transcripts(self):
        """æ£€æŸ¥å“ªäº›å½±ç‰‡å·²ç»æœ‰é€å­—ç¨¿"""
        print("\n" + "="*80)
        print("æ­¥éª¤ 2: æ£€æŸ¥å·²æœ‰é€å­—ç¨¿")
        print("="*80)
        
        print(f"\næ­£åœ¨æ‰«æç›®å½•: {self.transcripts_dir}")
        
        existing_transcripts = set()
        for txt_file in self.transcripts_dir.glob("*.txt"):
            video_id = txt_file.stem
            existing_transcripts.add(video_id)
        
        self.stats['videos_with_transcript'] = len(existing_transcripts)
        
        # æ ‡è®°å“ªäº›å½±ç‰‡éœ€è¦ç”Ÿæˆé€å­—ç¨¿
        videos_need_transcript = []
        
        for video_id, info in self.video_info.items():
            if video_id in existing_transcripts:
                info['has_transcript'] = True
            else:
                info['has_transcript'] = False
                videos_need_transcript.append(video_id)
        
        self.stats['videos_need_transcript'] = len(videos_need_transcript)
        
        print(f"\nâœ… æ£€æŸ¥å®Œæˆ:")
        print(f"  å·²æœ‰é€å­—ç¨¿: {self.stats['videos_with_transcript']:,} ä¸ª")
        print(f"  éœ€è¦ç”Ÿæˆ: {self.stats['videos_need_transcript']:,} ä¸ª")
        
        return videos_need_transcript
    
    def display_video_summary(self, top_n=20):
        """æ˜¾ç¤ºå½±ç‰‡ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*80)
        print("å½±ç‰‡ç»Ÿè®¡æ‘˜è¦")
        print("="*80)
        
        # æŒ‰è§‚çœ‹æ•°æ’åº
        sorted_videos = sorted(
            self.video_info.items(),
            key=lambda x: x[1]['view_count'],
            reverse=True
        )
        
        print(f"\nã€è§‚çœ‹æ•°æœ€å¤šçš„ {top_n} ä¸ªå½±ç‰‡ã€‘")
        print(f"{'æ’å':<6} {'å½±ç‰‡ID':<15} {'è§‚çœ‹æ•°':>10} {'è¯„è®ºæ•°':>8} {'é€å­—ç¨¿':>8} {'é¢‘é“':<20} {'æ ‡é¢˜':<40}")
        print("-" * 130)
        
        for i, (video_id, info) in enumerate(sorted_videos[:top_n], 1):
            has_transcript = "âœ“" if info.get('has_transcript', False) else "âœ—"
            channel = info['channel_title'][:18] if len(info['channel_title']) > 18 else info['channel_title']
            title = info['video_title'][:38] if len(info['video_title']) > 38 else info['video_title']
            
            print(f"{i:<6} {video_id:<15} {info['view_count']:>10,} "
                  f"{info['comment_count']:>8,} {has_transcript:>8} {channel:<20} {title:<40}")
        
        # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
        print(f"\nã€æŒ‰æœˆä»½ç»Ÿè®¡å½±ç‰‡æ•°ã€‘")
        monthly_counts = defaultdict(int)
        for video_id, info in self.video_info.items():
            publish_date = info['publish_date']
            if publish_date:
                month = publish_date[:7]  # YYYY-MM
                monthly_counts[month] += 1
        
        print(f"{'æœˆä»½':<10} {'å½±ç‰‡æ•°':>10}")
        print("-" * 25)
        for month in sorted(monthly_counts.keys()):
            print(f"{month:<10} {monthly_counts[month]:>10,}")
        
        # æŒ‰é¢‘é“ç»Ÿè®¡
        print(f"\nã€æŒ‰é¢‘é“ç»Ÿè®¡å½±ç‰‡æ•°ã€‘")
        channel_counts = defaultdict(int)
        channel_names = {}
        for video_id, info in self.video_info.items():
            channel = info['channel']
            channel_counts[channel] += 1
            channel_names[channel] = info['channel_title']
        
        sorted_channels = sorted(channel_counts.items(), key=lambda x: x[1], reverse=True)
        
        print(f"{'é¢‘é“ID':<30} {'é¢‘é“åç§°':<30} {'å½±ç‰‡æ•°':>10}")
        print("-" * 75)
        for channel_id, count in sorted_channels[:20]:
            channel_name = channel_names.get(channel_id, '')[:28]
            print(f"{channel_id:<30} {channel_name:<30} {count:>10,}")
    
    def batch_transcribe(self, model_size="base", max_videos=None, start_from=0, 
                        sort_by="view_count"):
        """
        æ‰¹é‡ç”Ÿæˆé€å­—ç¨¿
        
        Args:
            model_size: Whisperæ¨¡å‹å¤§å° (tiny, base, small, medium, large)
            max_videos: æœ€å¤šå¤„ç†çš„å½±ç‰‡æ•°é‡ (None = å…¨éƒ¨)
            start_from: ä»ç¬¬å‡ ä¸ªå½±ç‰‡å¼€å§‹ (ç”¨äºæ–­ç‚¹ç»­ä¼ )
            sort_by: æ’åºæ–¹å¼ (view_count, comment_count, publish_date)
        """
        print("\n" + "="*80)
        print("æ­¥éª¤ 3: æ‰¹é‡ç”Ÿæˆé€å­—ç¨¿")
        print("="*80)
        
        # è·å–éœ€è¦å¤„ç†çš„å½±ç‰‡åˆ—è¡¨
        videos_to_process = [
            (vid, info) for vid, info in self.video_info.items()
            if not info.get('has_transcript', False)
        ]
        
        # æŒ‰æŒ‡å®šæ–¹å¼æ’åº
        if sort_by == "view_count":
            videos_to_process.sort(key=lambda x: x[1]['view_count'], reverse=True)
            print(f"\næ’åºæ–¹å¼: æŒ‰è§‚çœ‹æ•°ï¼ˆä¼˜å…ˆå¤„ç†é«˜è§‚çœ‹æ•°å½±ç‰‡ï¼‰")
        elif sort_by == "comment_count":
            videos_to_process.sort(key=lambda x: x[1]['comment_count'], reverse=True)
            print(f"\næ’åºæ–¹å¼: æŒ‰è¯„è®ºæ•°ï¼ˆä¼˜å…ˆå¤„ç†é«˜è¯„è®ºæ•°å½±ç‰‡ï¼‰")
        elif sort_by == "publish_date":
            videos_to_process.sort(key=lambda x: x[1]['publish_date'], reverse=False)
            print(f"\næ’åºæ–¹å¼: æŒ‰å‘å¸ƒæ—¥æœŸï¼ˆä»æ—§åˆ°æ–°ï¼‰")
        
        # åº”ç”¨èµ·å§‹ä½ç½®å’Œæ•°é‡é™åˆ¶
        if start_from > 0:
            videos_to_process = videos_to_process[start_from:]
        
        if max_videos is not None:
            videos_to_process = videos_to_process[:max_videos]
        
        total = len(videos_to_process)
        
        if total == 0:
            print("\næ²¡æœ‰éœ€è¦å¤„ç†çš„å½±ç‰‡ï¼")
            return
        
        print(f"\nâš™ï¸  è½¬å½•è®¾ç½®:")
        print(f"   æ¨¡å‹å¤§å°: {model_size}")
        print(f"   å¾…å¤„ç†å½±ç‰‡: {total}")
        if start_from > 0:
            print(f"   èµ·å§‹ä½ç½®: ç¬¬ {start_from + 1} ä¸ª")
        
        # åˆ›å»ºè½¬å½•å™¨
        print(f"\næ­£åœ¨åˆå§‹åŒ– Whisper æ¨¡å‹ ({model_size})...")
        transcriber = YouTubeTranscriber(
            model_size=model_size,
            output_dir=str(self.transcripts_dir)
        )
        
        # å¼€å§‹æ‰¹é‡å¤„ç†
        print(f"\nå¼€å§‹æ‰¹é‡è½¬å½• {total} ä¸ªå½±ç‰‡...")
        print("=" * 80)
        
        success_count = 0
        error_count = 0
        skip_count = 0
        
        for i, (video_id, info) in enumerate(videos_to_process, 1):
            actual_index = start_from + i
            video_title = info['video_title'] or 'Unknown'
            channel = info['channel_title']
            view_count = info['view_count']
            comment_count = info['comment_count']
            publish_date = info['publish_date'][:10] if info['publish_date'] else 'Unknown'
            
            print(f"\n[{i}/{total}] (æ€»ç¬¬ {actual_index}) å½±ç‰‡ID: {video_id}")
            print(f"æ ‡é¢˜: {video_title}")
            print(f"é¢‘é“: {channel}")
            print(f"å‘å¸ƒæ—¥æœŸ: {publish_date}")
            print(f"è§‚çœ‹æ•°: {view_count:,} | è¯„è®ºæ•°: {comment_count:,}")
            print("-" * 80)
            
            # æ„å»ºYouTube URL
            video_url = info.get("url") or f"https://www.youtube.com/watch?v={video_id}"
            
            try:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
                output_file = self.transcripts_dir / f"{video_id}.txt"
                if output_file.exists():
                    print(f"âŠ™ è·³è¿‡ï¼ˆå·²å­˜åœ¨é€å­—ç¨¿ï¼‰")
                    skip_count += 1
                    continue
                
                # 1ï¸âƒ£ å…ˆæ£€æŸ¥çœŸäººå­—å¹•
                manual_subs = get_manual_subtitles(video_url)

                if manual_subs:
                    print("âœ“ å‘ç°çœŸäººå­—å¹•ï¼Œä½¿ç”¨åŸå§‹å­—å¹•")
                    info['transcript_source'] = 'manual'
                    
                    # ä¸‹è½½å­—å¹•
                    download_manual_subtitle(video_url, self.transcripts_dir / video_id)
                    
                    # æŸ¥æ‰¾ä¸‹è½½çš„ VTT æ–‡ä»¶
                    vtt_files = sorted(
                        self.transcripts_dir.glob(f"{video_id}.zh*.vtt"),
                        key=lambda p: p.stat().st_mtime,
                        reverse=True
                    )
                    if not vtt_files:
                        raise RuntimeError("ä¸­æ–‡å­—å¹•å­—å¹•ä¸‹è½½å¤±è´¥")
                    
                    # è½¬ä¸º TXT
                    vtt_to_txt(vtt_files[0], output_file)
                    
                    # å»é‡
                    clean_transcript_file(output_file)
                    
                    success_count += 1
                    continue

                # 2ï¸âƒ£ æ²¡æœ‰çœŸäººå­—å¹• â†’ Whisper fallback
                print("âŠ™ æ— çœŸäººå­—å¹•ï¼Œä½¿ç”¨ Whisper")
                transcriber.process_video(video_url, language="zh", keep_audio=False)
                if not output_file.exists():
                    raise RuntimeError("Whisper è½¬å½•å¤±è´¥ï¼Œæœªç”Ÿæˆé€å­—ç¨¿")
                info['transcript_source'] = 'whisper'
                clean_transcript_file(output_file)
                success_count += 1
                
            except KeyboardInterrupt:
                print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰¹é‡å¤„ç†")
                print(f"\nå·²å¤„ç†ç»Ÿè®¡:")
                print(f"  æˆåŠŸ: {success_count}")
                print(f"  å¤±è´¥: {error_count}")
                print(f"  è·³è¿‡: {skip_count}")
                print(f"\næç¤º: å¯ä»¥ä½¿ç”¨ start_from={actual_index} ç»§ç»­å¤„ç†")
                raise
                
            except Exception as e:
                print(f"âœ— å¤„ç†å¤±è´¥: {str(e)}")
                error_count += 1
            
            print("=" * 80)
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n" + "="*80)
        print("æ‰¹é‡è½¬å½•å®Œæˆï¼")
        print("="*80)
        print(f"\nå¤„ç†ç»“æœ:")
        print(f"  æ€»æ•°: {total}")
        print(f"  æˆåŠŸ: {success_count}")
        print(f"  å¤±è´¥: {error_count}")
        print(f"  è·³è¿‡: {skip_count}")
        
        if error_count > 0:
            print(f"\nâš ï¸  {error_count} ä¸ªå½±ç‰‡å¤„ç†å¤±è´¥")
            print(f"  å¯èƒ½åŸå› : å½±ç‰‡å·²åˆ é™¤ã€ç§äººå½±ç‰‡ã€åœ°åŒºé™åˆ¶ç­‰")
    
    def save_video_list(self, output_file):
        """ä¿å­˜å½±ç‰‡åˆ—è¡¨ï¼ˆå«é€å­—ç¨¿çŠ¶æ€ï¼‰"""
        print(f"\næ­£åœ¨ä¿å­˜å½±ç‰‡åˆ—è¡¨åˆ°: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            fieldnames = [
                'video_id',
                'video_title',
                'channel_id',
                'channel_title',
                'publish_date',
                'view_count',
                'like_count',
                'comment_count',
                'duration',
                'has_transcript',
                'youtube_url',
                'transcript_source',
            ]
            
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            # æŒ‰å‘å¸ƒæ—¥æœŸæ’åº
            sorted_videos = sorted(
                self.video_info.items(),
                key=lambda x: x[1]['publish_date'],
                reverse=False
            )
            
            for video_id, info in sorted_videos:
                writer.writerow({
                    'video_id': video_id,
                    'video_title': info['video_title'],
                    'channel_id': info['channel'],
                    'channel_title': info['channel_title'],
                    'publish_date': info['publish_date'][:10] if info['publish_date'] else '',
                    'view_count': info['view_count'],
                    'like_count': info['like_count'],
                    'comment_count': info['comment_count'],
                    'duration': info['duration'],
                    'has_transcript': 'Yes' if info.get('has_transcript', False) else 'No',
                    'youtube_url': f"https://www.youtube.com/watch?v={video_id}",
                    'transcript_source': info.get('transcript_source', '')
                })
        
        print(f"âœ… å·²ä¿å­˜ {len(self.video_info)} ä¸ªå½±ç‰‡çš„ä¿¡æ¯")
        return output_file


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("æ‰€æœ‰å½±ç‰‡æ‰¹é‡è½¬å½•å·¥å…· (2024-01 è‡³ 2025-10)")
    print("="*80)
    
    # è®¾ç½®å‚æ•°
    data_dir = './data'
    transcripts_dir = './transcripts_all'
    sample_json = "./ä¸­å¤©æ–°è_videos.json"
    start_date = "2024-01-01"  # å¼€å§‹æ—¥æœŸ
    end_date = "2025-10-31"    # ç»“æŸæ—¥æœŸ
    
    # ========================================
    # ğŸ”§ è½¬å½•å‚æ•°è®¾ç½®
    # ========================================
    WHISPER_MODEL = "medium"   # æ¨¡å‹é€‰æ‹©: tiny, base, small, medium, large
                             # tiny: æœ€å¿«ï¼Œå‡†ç¡®åº¦è¾ƒä½
                             # base: å¿«é€Ÿï¼Œå‡†ç¡®åº¦ä¸­ç­‰ï¼ˆæ¨èï¼‰
                             # small/medium/large: è¶Šæ¥è¶Šæ…¢ï¼Œä½†å‡†ç¡®åº¦æ›´é«˜
    
    MAX_VIDEOS = None          # ä¸€æ¬¡æœ€å¤šå¤„ç†å¤šå°‘ä¸ªå½±ç‰‡ (None = å…¨éƒ¨)
                             # å»ºè®®å…ˆè®¾ä¸º 10 æµ‹è¯•ï¼Œç¡®è®¤æ— è¯¯åè®¾ä¸º None
    
    START_FROM = 0           # ä»ç¬¬å‡ ä¸ªå½±ç‰‡å¼€å§‹ (0 = ä»å¤´å¼€å§‹)
                             # å¦‚æœä¸­æ–­äº†ï¼Œå¯ä»¥è®¾ç½®è¿™ä¸ªå‚æ•°ç»§ç»­å¤„ç†
    
    SORT_BY = "view_count"   # æ’åºæ–¹å¼: view_count, comment_count, publish_date
                             # view_count: ä¼˜å…ˆå¤„ç†é«˜è§‚çœ‹æ•°å½±ç‰‡
                             # comment_count: ä¼˜å…ˆå¤„ç†é«˜è¯„è®ºæ•°å½±ç‰‡
                             # publish_date: æŒ‰æ—¶é—´é¡ºåºå¤„ç†
    
    print(f"\nâš™ï¸  å‚æ•°è®¾ç½®:")
    print(f"   æ•°æ®ç›®å½•: {data_dir}")
    print(f"   è¾“å‡ºç›®å½•: {transcripts_dir}")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
    print(f"   Whisperæ¨¡å‹: {WHISPER_MODEL}")
    print(f"   æœ€å¤§å¤„ç†æ•°: {MAX_VIDEOS if MAX_VIDEOS else 'å…¨éƒ¨'}")
    print(f"   èµ·å§‹ä½ç½®: ç¬¬ {START_FROM + 1} ä¸ª")
    print(f"   æ’åºæ–¹å¼: {SORT_BY}")
    
    # åˆ›å»ºè½¬å½•å™¨
    processor = AllVideoTranscriber(data_dir, transcripts_dir, start_date, end_date)
    
    # æ‰§è¡Œæµç¨‹
    print("\n" + "="*80)
    print("å¼€å§‹å¤„ç†æµç¨‹")
    print("="*80)
    
    # æ­¥éª¤1: åŠ è½½å½±ç‰‡åˆ—è¡¨
    num_videos = processor.load_videos_from_sample_json(sample_json)
    
    if num_videos == 0:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•å½±ç‰‡ï¼Œç¨‹åºé€€å‡º")
        return None
    
    # æ­¥éª¤2: æ£€æŸ¥å·²æœ‰é€å­—ç¨¿
    videos_need_transcript = processor.check_existing_transcripts()
    
    # æ­¥éª¤3: æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    processor.display_video_summary(top_n=30)
    
    # æ­¥éª¤4: ä¿å­˜å½±ç‰‡åˆ—è¡¨
    video_list_file = processor.save_video_list(
        './all_videos_list_2024-2025.csv'
    )
    
    print(f"\nå·²ä¿å­˜å½±ç‰‡åˆ—è¡¨: {video_list_file}")
    
    # æ­¥éª¤5: è¯¢é—®æ˜¯å¦å¼€å§‹è½¬å½•
    print("\n" + "="*80)
    print("å‡†å¤‡å¼€å§‹æ‰¹é‡è½¬å½•")
    print("="*80)
    
    print(f"\nå°†è¦å¤„ç† {processor.stats['videos_need_transcript']} ä¸ªå½±ç‰‡")
    print(f"ä½¿ç”¨æ¨¡å‹: {WHISPER_MODEL}")
    print(f"æ’åºæ–¹å¼: {SORT_BY}")
    
    if MAX_VIDEOS:
        print(f"æœ¬æ¬¡æœ€å¤šå¤„ç†: {MAX_VIDEOS} ä¸ª")
    
    response = input("\næ˜¯å¦å¼€å§‹è½¬å½•? (y/N): ").strip().lower()
    
    if response == 'y':
        try:
            processor.batch_transcribe(
                model_size=WHISPER_MODEL,
                max_videos=MAX_VIDEOS,
                start_from=START_FROM,
                sort_by=SORT_BY
            )
            
            print("\n" + "="*80)
            print("âœ… å…¨éƒ¨å®Œæˆï¼")
            print("="*80)
            
            print(f"\né€å­—ç¨¿å·²ä¿å­˜åˆ°: {transcripts_dir}")
            print(f"æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨è¿™äº›é€å­—ç¨¿è¿›è¡Œåˆ†æï¼")
            
        except KeyboardInterrupt:
            print("\n\nç¨‹åºå·²ä¸­æ–­")
        except Exception as e:
            print(f"\nâœ— å‘ç”Ÿé”™è¯¯: {str(e)}")
    else:
        print("\nå·²å–æ¶ˆè½¬å½•")
        print(f"å½±ç‰‡åˆ—è¡¨å·²ä¿å­˜åˆ°: {video_list_file}")
        print("éœ€è¦æ—¶å¯ä»¥é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
    
    return processor


if __name__ == "__main__":
    try:
        processor = main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œç¨‹åºå·²å®‰å…¨é€€å‡º")
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

